{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tarfile\n",
    "\n",
    "Spam_path = os.path.join(\"datasets\",\"spams\")\n",
    "ham_dir = r\"C:\\Users\\vikranth\\ml\\DOING\\datasets\\spams\\easy_ham\"\n",
    "spam_dir = r\"C:\\Users\\vikranth\\ml\\DOING\\datasets\\spams\\spam\"\n",
    "\n",
    "ham_files = [name for name in sorted(os.listdir(ham_dir))if len(name)>20]\n",
    "spam_files = [name for name in sorted(os.listdir(spam_dir))if len(name)>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ham_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.parser\n",
    "import email.policy\n",
    "\n",
    "def load_mails(is_spam , filename, spam_path = Spam_path):\n",
    "  dir = \"spam\" if is_spam else \"easy_ham\"\n",
    "  with open(os.path.join(spam_path, dir, filename), \"rb\")as f:\n",
    "    return email.parser.BytesParser(policy=email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails = [load_mails(is_spam=False, filename=name) for name in ham_files]\n",
    "spam_emails = [load_mails(is_spam=True, filename=name) for name in spam_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin A posted:\n",
      "Tassos Papadopoulos, the Greek sculptor behind the plan, judged that the\n",
      " limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\n",
      " Mount Athos monastic community, was ideal for the patriotic sculpture. \n",
      " \n",
      " As well as Alexander's granite features, 240 ft high and 170 ft wide, a\n",
      " museum, a restored amphitheatre and car park for admiring crowds are\n",
      "planned\n",
      "---------------------\n",
      "So is this mountain limestone or granite?\n",
      "If it's limestone, it'll weather pretty fast.\n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "4 DVDs Free +s&p Join Now\n",
      "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[1].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help wanted.  We are a 14 year old fortune 500 company, that is\n",
      "growing at a tremendous rate.  We are looking for individuals who\n",
      "want to work from home.\n",
      "\n",
      "This is an opportunity to make an excellent income.  No experience\n",
      "is required.  We will train you.\n",
      "\n",
      "So if you are looking to be employed from home with a career that has\n",
      "vast opportunities, then go:\n",
      "\n",
      "http://www.basetel.com/wealthnow\n",
      "\n",
      "We are looking for energetic and self motivated people.  If that is you\n",
      "than click on the link and fill out the form, and one of our\n",
      "employement specialist will contact you.\n",
      "\n",
      "To be removed from our link simple go to:\n",
      "\n",
      "http://www.basetel.com/remove.html\n",
      "\n",
      "\n",
      "4139vOLW7-758DoDY1425FRhM1-764SMFc8513fCsLl40\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[6].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_email_structure(email):\n",
    "  if isinstance(email, str):\n",
    "    return email\n",
    "  payload = email.get_payload()\n",
    "  if isinstance(payload, list):\n",
    "    return \"multipart({})\".format(\", \".join([\n",
    "      get_email_structure(sub_email)\n",
    "      for sub_email in payload\n",
    "    ]))\n",
    "  else:\n",
    "    return email.get_content_type()\n",
    "  \n",
    "def structures_counter(emails):\n",
    "  structures = Counter()\n",
    "  for email in emails:\n",
    "    structure = get_email_structure(email)\n",
    "    structures[structure] += 1\n",
    "  return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2408),\n",
       " ('multipart(text/plain, application/pgp-signature)', 66),\n",
       " ('multipart(text/plain, text/html)', 8),\n",
       " ('multipart(text/plain, text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, text/enriched)', 1),\n",
       " ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n",
       " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain, video/mng)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain))', 1),\n",
       " ('multipart(text/plain, application/x-pkcs7-signature)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n",
       "  1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n",
       "  1),\n",
       " ('multipart(text/plain, application/x-java-applet)', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 218),\n",
       " ('text/html', 183),\n",
       " ('multipart(text/plain, text/html)', 45),\n",
       " ('multipart(text/html)', 20),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 1),\n",
       " ('multipart(text/html, text/plain)', 1),\n",
       " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n",
       " ('multipart/alternative', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(spam_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path : <Thecashsystem@firemail.de>\n",
      "Delivered-To : zzzz@localhost.spamassassin.taint.org\n",
      "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id 3453043F99\tfor <zzzz@localhost>; Thu, 22 Aug 2002 11:58:24 -0400 (EDT)\n",
      "Received : from mail.webnote.net [193.120.211.219]\tby localhost with POP3 (fetchmail-5.9.0)\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 16:58:24 +0100 (IST)\n",
      "Received : from mailbox-13.st1.spray.net (mailbox-13.st1.spray.net [212.78.202.113])\tby webnote.net (8.9.3/8.9.3) with ESMTP id QAA05573\tfor <zzzz@spamassassin.taint.org>; Thu, 22 Aug 2002 16:55:29 +0100\n",
      "Received : from freesource (user-24-214-168-210.knology.net [24.214.168.210])\tby mailbox-13.st1.spray.net (Postfix) with ESMTP\tid ADDD03E25C; Thu, 22 Aug 2002 17:50:55 +0200 (DST)\n",
      "Message-ID : <413-220028422154219900@freesource>\n",
      "X-Priority : 1\n",
      "To : 1 <thecashsystem@firemail.de>\n",
      "From : TheCashSystem <Thecashsystem@firemail.de>\n",
      "Subject : RE: Your Bank Account Information \n",
      "Date : Thu, 22 Aug 2002 10:42:19 -0500\n",
      "MIME-Version : 1.0\n",
      "Content-type : text/plain; charset=\"US-ASCII\"\n",
      "X-MIME-Autoconverted : from quoted-printable to 8bit by webnote.net id QAA05573\n",
      "Content-Transfer-Encoding : 8bit\n"
     ]
    }
   ],
   "source": [
    "for header, value in spam_emails[5].items():\n",
    "  print(header,\":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Life Insurance - Why Pay More?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_emails[0][\"Subject\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(ham_emails + spam_emails, dtype= object)\n",
    "Y = np.array([0]*len(ham_emails)+[1]*len(spam_emails))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def html_to_plain_text(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML>\n",
      "<BODY BGCOLOR=\"#ffffff\">\n",
      "<P>\n",
      "<<HTML>\n",
      "<TABLE WIDTH=400 BORDER=0 CELLPADDING=0 CELLSPACING=0>\n",
      "  <TR>\n",
      "    <TD ALIGN=\"LEFT\" VALIGN=\"TOP\"><FONT FACE=\"Tahoma, Arial, Verdana\" SIZE=2></FONT>\n",
      "      <H2>\n",
      "\t<FONT COLOR=\"#FF0000\">GET HIGH...LEGALLY!</FONT>\n",
      "      </H2>\n",
      "      <P>\n",
      "      <B>IT REALLY WORKS!<BR>\n",
      "      PASSES ALL DRUG TESTS!<BR>\n",
      "      EXTREMELY POTENT!</B>\n",
      "      <P>\n",
      "      <A HREF=\"http://www.greenmatrix.net/herb/index.html\"><B>CLICK HERE for more\n",
      "      info on Salvia Divinorum</B></A>\n",
      "      <P>\n",
      "      <B> <A HREF=\"http://www.greenmatrix.net/herb/5x.html\">CLICK HERE for SALVIA\n",
      "      5X EXTRACT!</A> <BR>\n",
      "      <FONT COLOR=\"#FF0000\"><BIG>WARNING...VERY POTENT!</BIG></FONT></B>\n",
      "      <P>\n",
      "      <B> <A HREF=\"http://www.greenmatrix.net/herb/13x.html\">CLICK HERE for SALVIA\n",
      "      13X</A>. The most POTENT, LEGAL, SMOKABLE herb on the planet! 13 times the\n",
      "      power of Salvia Divinorum!<BR>\n",
      "      <FONT COLOR=\"#FF0000\"><BIG>WARNING...EXTREMELY POTENT!</BIG></FONT></B>\n",
      "      <P>\n",
      "      <P>\n",
      "    ...\n"
     ]
    }
   ],
   "source": [
    "html_spam_emails = [email for email in x_train[y_train==1]\n",
    "                    if get_email_structure(email)==\"text/html\"]\n",
    "sample_html_spam = html_spam_emails[0]\n",
    "print(sample_html_spam.get_content()[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tGET HIGH...LEGALLY!\n",
      "      IT REALLY WORKS!\n",
      "      PASSES ALL DRUG TESTS!\n",
      "      EXTREMELY POTENT!\n",
      "       HYPERLINK CLICK HERE for more\n",
      "      info on Salvia Divinorum\n",
      "        HYPERLINK CLICK HERE for SALVIA\n",
      "      5X EXTRACT!\n",
      "      WARNING...VERY POTENT!\n",
      "        HYPERLINK CLICK HERE for SALVIA\n",
      "      13X. The most POTENT, LEGAL, SMOKABLE herb on the planet! 13 times the\n",
      "      power of Salvia Divinorum!\n",
      "      WARNING...EXTREMELY POTENT!\n",
      "      Removal Information:\n",
      "      We are strongly against sending unsolicited emails to those who do not wish\n",
      "      to receive our special mailings. You have opted in to one or more of our\n",
      "      affiliate sites requesting to be notified of any special offers we may run\n",
      "      from time to time. This is NOT unsolicited email. If you do not wish to receive\n",
      "      further mailings, please\n",
      "       HYPERLINK click here to be removed\n",
      "      from the list. Please accept our apologies if you have been sent this\n",
      "      email in error. We honor all removal requests within  ...\n"
     ]
    }
   ],
   "source": [
    "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "  html = None\n",
    "  for part in email.walk():\n",
    "    ctype = part.get_content_type()\n",
    "    if not ctype in (\"text/plain\",\"text/html\"):\n",
    "      continue\n",
    "    try:\n",
    "      content = part.get_content()\n",
    "    except:\n",
    "      content = str(part.get_payload())\n",
    "    if ctype == \"text/plain\":\n",
    "      return content\n",
    "    else:\n",
    "      html = content\n",
    "  if html:\n",
    "    return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tGET HIGH...LEGALLY!\n",
      "      IT REALLY WORKS!\n",
      "      PASSES ALL DRUG TESTS!\n",
      "      EXTREMELY POTENT!\n",
      "   ...\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample_html_spam)[:100],\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations => comput\n",
      "Computation => comput\n",
      "Computing => comput\n",
      "Computed => comput\n",
      "Compute => comput\n",
      "Compulsive => compuls\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
    "        print(word, \"=>\", stemmer.stem(word))\n",
    "except ImportError:\n",
    "    print(\"Error: stemming requires the NLTK module.\")\n",
    "    stemmer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['github.com', 'https://youtu.be/7Pq-S557XQU?t=3m32s']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import urlextract # may require an Internet connection to download root domain names\n",
    "    \n",
    "    url_extractor = urlextract.URLExtract()\n",
    "    print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))\n",
    "except ImportError:\n",
    "    print(\"Error: replacing URLs requires the urlextract module.\")\n",
    "    url_extractor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailToWordConverterTransformer(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, strip_headers = True, lower_case = True, remove_punctation = True,\n",
    "               replace_urls = True, replace_numbers= True, stemming = True):\n",
    "    self.strip_headers = strip_headers\n",
    "    self.lower_case = lower_case\n",
    "    self.remove_punctation = remove_punctation\n",
    "    self.replace_urls = replace_urls\n",
    "    self.replace_numbers = replace_numbers\n",
    "    self.stemming = stemming\n",
    "\n",
    "  def fit(self, x, y=None):\n",
    "    return self\n",
    "  \n",
    "  def transform(self, x, y=None):\n",
    "    x_transform = []\n",
    "\n",
    "    for email in x:\n",
    "      text = email_to_text(email) or \"\"\n",
    "      if self.lower_case:\n",
    "        text = text.lower()\n",
    "\n",
    "      if self.replace_urls and url_extractor is not None:\n",
    "        urls = list(set(url_extractor.find_urls(text)))\n",
    "        urls.sort(key=lambda url: len(url),reverse=True)\n",
    "        for url in urls:\n",
    "          text= text.replace(url, \"URL\")\n",
    "\n",
    "      if self.replace_numbers:\n",
    "        text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
    "\n",
    "      if self.remove_punctation:\n",
    "        text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "      \n",
    "      word_counts = Counter(text.split())\n",
    "      if self.stemming or stemmer is not None:\n",
    "        stemmed_word_counts = Counter()\n",
    "        for word, count in word_counts.items():\n",
    "          stemmed_word = stemmer.stem(word)\n",
    "          stemmed_word_counts[stemmed_word] += count\n",
    "        word_counts = stemmed_word_counts\n",
    "      x_transform.append(word_counts)\n",
    "    return np.array(x_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'chuck': 1, 'murcko': 1, 'wrote': 1, 'stuff': 1, 'yawn': 1, 'r': 1}),\n",
       "       Counter({'the': 11, 'of': 9, 'and': 8, 'all': 3, 'christian': 3, 'to': 3, 'by': 3, 'jefferson': 2, 'i': 2, 'have': 2, 'superstit': 2, 'one': 2, 'on': 2, 'been': 2, 'ha': 2, 'half': 2, 'rogueri': 2, 'teach': 2, 'jesu': 2, 'some': 1, 'interest': 1, 'quot': 1, 'url': 1, 'thoma': 1, 'examin': 1, 'known': 1, 'word': 1, 'do': 1, 'not': 1, 'find': 1, 'in': 1, 'our': 1, 'particular': 1, 'redeem': 1, 'featur': 1, 'they': 1, 'are': 1, 'alik': 1, 'found': 1, 'fabl': 1, 'mytholog': 1, 'million': 1, 'innoc': 1, 'men': 1, 'women': 1, 'children': 1, 'sinc': 1, 'introduct': 1, 'burnt': 1, 'tortur': 1, 'fine': 1, 'imprison': 1, 'what': 1, 'effect': 1, 'thi': 1, 'coercion': 1, 'make': 1, 'world': 1, 'fool': 1, 'other': 1, 'hypocrit': 1, 'support': 1, 'error': 1, 'over': 1, 'earth': 1, 'six': 1, 'histor': 1, 'american': 1, 'john': 1, 'e': 1, 'remsburg': 1, 'letter': 1, 'william': 1, 'short': 1, 'again': 1, 'becom': 1, 'most': 1, 'pervert': 1, 'system': 1, 'that': 1, 'ever': 1, 'shone': 1, 'man': 1, 'absurd': 1, 'untruth': 1, 'were': 1, 'perpetr': 1, 'upon': 1, 'a': 1, 'larg': 1, 'band': 1, 'dupe': 1, 'import': 1, 'led': 1, 'paul': 1, 'first': 1, 'great': 1, 'corrupt': 1}),\n",
       "       Counter({'url': 4, 's': 3, 'group': 3, 'to': 3, 'in': 2, 'forteana': 2, 'martin': 2, 'an': 2, 'and': 2, 'we': 2, 'is': 2, 'yahoo': 2, 'unsubscrib': 2, 'y': 1, 'adamson': 1, 'wrote': 1, 'for': 1, 'altern': 1, 'rather': 1, 'more': 1, 'factual': 1, 'base': 1, 'rundown': 1, 'on': 1, 'hamza': 1, 'career': 1, 'includ': 1, 'hi': 1, 'belief': 1, 'that': 1, 'all': 1, 'non': 1, 'muslim': 1, 'yemen': 1, 'should': 1, 'be': 1, 'murder': 1, 'outright': 1, 'know': 1, 'how': 1, 'unbias': 1, 'memri': 1, 'don': 1, 't': 1, 'html': 1, 'rob': 1, 'sponsor': 1, 'number': 1, 'dvd': 1, 'free': 1, 'p': 1, 'join': 1, 'now': 1, 'from': 1, 'thi': 1, 'send': 1, 'email': 1, 'egroup': 1, 'com': 1, 'your': 1, 'use': 1, 'of': 1, 'subject': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_few = x_train[:3]\n",
    "x_few_wordcounts = EmailToWordConverterTransformer().fit_transform(x_few)\n",
    "x_few_wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 20 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transform =  WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "x_few_vector = vocab_transform.fit_transform(x_few_wordcounts)\n",
    "x_few_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [99, 11,  9,  8,  3,  1,  3,  1,  3,  2,  3],\n",
       "       [67,  0,  1,  2,  3,  4,  1,  2,  0,  1,  0]], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_few_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'of': 2,\n",
       " 'and': 3,\n",
       " 'to': 4,\n",
       " 'url': 5,\n",
       " 'all': 6,\n",
       " 'in': 7,\n",
       " 'christian': 8,\n",
       " 'on': 9,\n",
       " 'by': 10}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transform.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocessPipeline= Pipeline([\n",
    "  (\"email_to_wordcount\",EmailToWordConverterTransformer()),\n",
    "  (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "x_train_transformed = preprocessPipeline.fit_transform(x_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
